{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K0nmeRk7KtY"
      },
      "source": [
        "<center><font size = '+3'>Self-Constructed CART</font></center>\n",
        "<br>\n",
        "<center><font size = '+2'>an ensemble learning project</font></center>\n",
        "<center><font size = '+1'>by</font></center>\n",
        "<center><font size = '+1'>Congjie AN, Jialin DU, Zeli PAN, Yifan WANG</font></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ZQRi7r8xCn"
      },
      "source": [
        "## Construction of CART\n",
        "\n",
        "In this part we implement a CART from scratch ***using only Numpy***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "f6Q-n8t7639Y"
      },
      "outputs": [],
      "source": [
        "# import required packages\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "-JNXNfFisvH7"
      },
      "outputs": [],
      "source": [
        "# define the function to calculate impurity\n",
        "\n",
        "def impurity(criterion, labels, decimal = None):\n",
        "  assert criterion in ['gini', 'mse', 'entropy'], f\"Error: criterion \\'{criterion}\\' is not supported.\"\n",
        "  labels = np.array(labels)\n",
        "  if criterion == 'gini': # Gini impurity for classification tasks\n",
        "    length = float(len(labels)) # pre-define to save runtime\n",
        "    res =  1.0 - sum([ ( float(len(labels[labels == label])) / length ) ** 2.0 for label in np.unique(labels) ])\n",
        "  elif criterion == 'mse': # MSE for regression tasks\n",
        "    res = np.mean( (labels - np.mean(labels)) ** 2.0 )\n",
        "  else: # Entropy impurity for classification tasks\n",
        "    res = list()\n",
        "    length = float(len(labels)) # pre-define to save runtime\n",
        "    p = np.array([float(len(labels[labels == label])) / length for label in np.unique(labels)])\n",
        "    res = sum( p * np.log2(p) * (-1) )\n",
        "  # set the result to targeted decimal\n",
        "  return res if decimal == None else round(res, decimal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "H5OJBpc23TL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb917bb-676c-41e7-925a-8fe0d214ce19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7\n",
            "1.85\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# test the impurity function\n",
        "\n",
        "lst1 = [1, 1, 1, 1, 1, 1]\n",
        "lst2 = [1, 1, 1, 2, 2, 2]\n",
        "\n",
        "assert impurity('gini', lst1) == 0.0\n",
        "assert impurity('gini', lst2) == 0.5\n",
        "assert impurity('entropy', lst1) == 0.0\n",
        "assert impurity('entropy', lst2) == 1.0\n",
        "assert impurity('mse', lst1) == 0.0\n",
        "assert impurity('mse', lst2) == 0.25\n",
        "\n",
        "del lst1\n",
        "del lst2\n",
        "\n",
        "lst3 = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\n",
        "print(impurity('gini', lst3, 2))\n",
        "print(impurity('entropy', lst3, 2))\n",
        "print(impurity('mse', lst3, 2))\n",
        "del lst3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "7MF1o6sw9qKj"
      },
      "outputs": [],
      "source": [
        "# define the node class\n",
        "\n",
        "class Node(object):\n",
        "  def __init__(self, criterion, max_depth, min_samples, min_gain):\n",
        "    # these are the basic attributes of a node\n",
        "    self.label = None # the label assigned to the samples in this node\n",
        "    self.depth = None # the depth of this node\n",
        "    self.num_samples = None # the number of samples in this node\n",
        "    self.impurity = None # the impurity of the samples in this node\n",
        "    # these are the attributes related to the tree\n",
        "    self.criterion = criterion\n",
        "    self.max_depth = max_depth # max depth of a tree\n",
        "    self.min_samples = min_samples # minimum number of samples in a node\n",
        "    self.min_gain = min_gain # minimum impurity gain from a split\n",
        "    # these are the attributes related to the splitment of a node\n",
        "    self.is_leaf = False # whether a node is a leaf\n",
        "    self.split_feature = None # the feature that used to split\n",
        "    self.split_threshold = None # the point to split\n",
        "    self.child_left = None # the child node on the left\n",
        "    self.left_num_samples = None # the number of sample of the left child\n",
        "    self.child_right = None # the child node on the right\n",
        "    self.right_num_samples = None # the number of sample of the right child\n",
        "    self.split_gain = None # the impurity gain of the split\n",
        "\n",
        "  # a function to grow the tree (split the node)\n",
        "  def grow(self, features, targets):\n",
        "    # set number of samples for current node\n",
        "    self.num_samples = features.shape[0]\n",
        "    # calculate the impurity of this node\n",
        "    self.impurity = impurity(criterion = self.criterion, labels = targets, decimal = 4)\n",
        "    # set label of samples for current node\n",
        "    # if all the samples belong to one class, stop growing from this node\n",
        "    if len(np.unique(targets)) == 1:\n",
        "      self.label = targets[0]\n",
        "      self.is_leaf = True\n",
        "      return\n",
        "    # else, choose the most common class as node label\n",
        "    if self.criterion in ['gini', 'entropy']: # classification\n",
        "      unique_labels, labels_counts = np.unique(targets, return_counts = True)\n",
        "      self.label = unique_labels[np.argmax(labels_counts)]\n",
        "      del unique_labels, labels_counts\n",
        "    else: # regression\n",
        "      self.label = np.mean(targets)\n",
        "    # if the depth of the node reaches max_depth, stop growing\n",
        "    if self.depth == self.max_depth:\n",
        "      self.is_leaf = True\n",
        "      return\n",
        "    \n",
        "    # Decide using which feature and which threshold to split the node\n",
        "    for feature_index in range(features.shape[1]):\n",
        "      feature_values = np.unique(features[:, feature_index])\n",
        "      feature_values = np.sort(feature_values) # sorted unique values from this feature\n",
        "      thresholds = (feature_values[:-1] + feature_values[1:]) / 2.0\n",
        "      for threshold in thresholds:\n",
        "        # calculate the attributes of the left child node\n",
        "        targets_left = targets[features[:, feature_index] <= threshold]\n",
        "        impurity_left = impurity(self.criterion, targets_left, 4)\n",
        "        ratio_left = float(len(targets_left)) / float(self.num_samples)\n",
        "        # calculate the attributes of the right child node\n",
        "        targets_right = targets[features[:, feature_index] > threshold]\n",
        "        impurity_right = impurity(self.criterion, targets_right, 4)\n",
        "        ratio_right = float(len(targets_right)) / float(self.num_samples)\n",
        "        # calculate the impurity gain from this split\n",
        "        impurity_gain = self.impurity - (ratio_left * impurity_left + ratio_right * impurity_right)\n",
        "        # save the best way of split\n",
        "        if (self.split_gain == None) or (impurity_gain > self.split_gain):\n",
        "          self.split_gain = impurity_gain\n",
        "          self.split_feature = feature_index\n",
        "          self.split_threshold = threshold\n",
        "          self.left_num_samples = len(targets_left)\n",
        "          self.right_num_samples = len(targets_right)\n",
        "    # check whether this split meet stop conditions\n",
        "    if self.min_samples != None: # stop by min_samples\n",
        "      if (self.left_num_samples < self.min_samples) or (self.right_num_samples < self.min_samples):\n",
        "        self.is_leaf = True\n",
        "        return\n",
        "    if self.min_gain != None: # stop by min_gain\n",
        "      if self.split_gain < self.min_gain:\n",
        "        self.is_leaf = True\n",
        "        return\n",
        "    # pass the stop conditions, begin splitting\n",
        "    # split the left node\n",
        "    features_left = features[features[:, self.split_feature] <= self.split_threshold]\n",
        "    targets_left = targets[features[:, self.split_feature] <= self.split_threshold]\n",
        "    self.child_left = Node(criterion = self.criterion, max_depth = self.max_depth, min_samples = self.min_samples, min_gain = self.min_gain)\n",
        "    self.child_left.depth = self.depth + 1\n",
        "    self.child_left.grow(features_left, targets_left)\n",
        "    # split the right node\n",
        "    features_right = features[features[:, self.split_feature] > self.split_threshold]\n",
        "    targets_right = targets[features[:, self.split_feature] > self.split_threshold]\n",
        "    self.child_right = Node(criterion = self.criterion, max_depth = self.max_depth, min_samples = self.min_samples, min_gain = self.min_gain)\n",
        "    self.child_right.depth = self.depth + 1\n",
        "    self.child_right.grow(features_right, targets_right)\n",
        "  \n",
        "  # a function to make classification for a given instance\n",
        "  def predict(self, row):\n",
        "    if self.is_leaf == False:\n",
        "      if row[self.split_feature] <= self.split_threshold:\n",
        "        return self.child_left.predict(row)\n",
        "      else:\n",
        "        return self.child_right.predict(row)\n",
        "    else: \n",
        "      return self.label\n",
        "\n",
        "  # a function to print the label, split feature and threshold\n",
        "  def show(self, condition):\n",
        "    if self.is_leaf == False:\n",
        "      print('  ' * self.depth + condition + f\"if X[{str(self.split_feature)}] <= {str(self.split_threshold)}\")\n",
        "      self.child_left.show('then ')\n",
        "      self.child_right.show('else ')\n",
        "    else:\n",
        "      if self.criterion == 'mse':\n",
        "        print('  ' * self.depth + f\"Value: {str(self.label)}, Num_samples: {str(self.num_samples)}\")\n",
        "      else:\n",
        "        print('  ' * self.depth + f\"Class: {str(self.label)}, Num_samples: {str(self.num_samples)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "aPEjtdG97fIy"
      },
      "outputs": [],
      "source": [
        "# define the CART class\n",
        "\n",
        "# Define a class for the whole CART\n",
        "class CART(object):\n",
        "  def __init__(self, criterion = 'gini', max_depth = 5, min_samples = None, min_gain = None, print_begin = False):\n",
        "    assert criterion in ['gini', 'mse', 'entropy'], f\"Error: criterion \\'{criterion}\\' is not supported.\"\n",
        "    self.criterion = criterion # the impurity function used\n",
        "    self.max_depth = max_depth # the maximum depth of the tree\n",
        "    self.min_samples = min_samples # the minimum number of samples in a node\n",
        "    self.min_gain = min_gain # the minimum split impurity gain\n",
        "    self.root = Node(criterion = self.criterion, max_depth = self.max_depth, min_samples = self.min_samples, min_gain = self.min_gain)\n",
        "    self.root.depth = 0\n",
        "    if print_begin == True:\n",
        "      print(\"Tree initialized, make sure that your criterion is \\'gini\\' or \\'entropy\\' for classification tasks, and \\'mse\\' for regression tasks.\")\n",
        "\n",
        "  # the function to train the CART\n",
        "  def fit(self, features, targets):\n",
        "    self.root.grow(features, targets)\n",
        "\n",
        "  # the function to make predictions\n",
        "  def predict(self, features):\n",
        "    return np.array([self.root.predict(row) for row in features])\n",
        "\n",
        "  # the function to print the CART\n",
        "  def show(self):\n",
        "    return self.root.show('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPFRBYBlecpE"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "- We evaluate our CART in classification tasks using the Iris, Wine and Breast Cancer Wisconsin (Diagnostic) datasets.\n",
        "- We evaluate our CART in regression tasks using the datasets we generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "HIC2oitEecpE"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def classification_evaluation(sklearn_dataset, print_tree = True):\n",
        "  assert sklearn_dataset in ['iris', 'wine', 'breast_cancer'], f\"Error: dataset \\'{sklearn_dataset}\\' is not supported.\"\n",
        "  print(f\"Begin evaluation on {sklearn_dataset} dataset.\")\n",
        "\n",
        "  # load the chosen dataset\n",
        "  if sklearn_dataset == 'iris':\n",
        "    dataset = load_iris()\n",
        "  elif sklearn_dataset == 'wine':\n",
        "    dataset = load_wine()\n",
        "  else:\n",
        "    dataset = load_breast_cancer()\n",
        "  X, y = dataset.data, dataset.target\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 513)\n",
        "\n",
        "  # predict using sklearn\n",
        "  cls = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5)\n",
        "  cls.fit(X_train, y_train)\n",
        "  pred = cls.predict(X_test)\n",
        "  f1score = f1_score(y_test, pred, average = 'weighted')\n",
        "  print(f\"\\tSklearn Library Tree Prediction f1 score: {f1score}\")\n",
        "\n",
        "  # predict using self-defined CART\n",
        "  cls = CART(criterion = 'entropy', max_depth = 5)\n",
        "  cls.fit(X_train, y_train)\n",
        "  pred = cls.predict(X_test)\n",
        "  f1score = f1_score(y_test, pred, average = 'weighted')\n",
        "  print(f\"\\tCART Prediction f1 score: {f1score}\")\n",
        "  \n",
        "  # print the CART\n",
        "  if print_tree == True:\n",
        "    cls.show()\n",
        "\n",
        "def regression_evaluation(print_tree = True):\n",
        "  print(\"Begin evaluation on regression tasks.\")\n",
        "  # Generate a random dataset\n",
        "  parameters = np.random.randint(low = 1, high = 6, size = 5)\n",
        "  c = np.random.randint(low = -50, high = 50)\n",
        "  X = np.random.randint(low = 1, high = 101, size=(500, 5))\n",
        "  error = np.random.normal(loc = 0, scale = 1, size = 500) * np.mean(X, axis=1)\n",
        "  y = X[:, 0] * parameters[0]\n",
        "  for i in [1, 2, 3, 4]:\n",
        "    y += X[:, i] * parameters[i]\n",
        "  y += c\n",
        "  y = y.astype('float64')\n",
        "  y += error\n",
        "  del parameters, c, error\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 513)\n",
        "  \n",
        "  # predict using sklearn\n",
        "  reg = tree.DecisionTreeRegressor(max_depth = 5)\n",
        "  reg.fit(X_train, y_train)\n",
        "  pred = reg.predict(X_test)\n",
        "  mse = np.mean((pred - y_test) ** 2)\n",
        "  print(f\"\\tSklearn Library Tree Prediction MSE: {mse}\")\n",
        "  \n",
        "  # predict using CART\n",
        "  reg = CART(criterion = 'mse', max_depth = 5)\n",
        "  reg.fit(X_train, y_train)\n",
        "  pred = reg.predict(X_test)\n",
        "  mse = np.mean((pred - y_test) ** 2)\n",
        "  print(f\"\\tCart Prediction MSE: {mse}\")\n",
        "\n",
        "  # print the CART\n",
        "  if print_tree == True:\n",
        "    reg.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_evaluation('iris', print_tree = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqVVV-ALMgJx",
        "outputId": "27fd9cb8-d9f6-4de2-b5b2-3f6037104aa3"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin evaluation on iris dataset.\n",
            "\tSklearn Library Tree Prediction f1 score: 0.9738894018672412\n",
            "\tCART Prediction f1 score: 0.9738894018672412\n",
            "if X[2] <= 2.45\n",
            "  Class: 0, Num_samples: 35\n",
            "  else if X[3] <= 1.65\n",
            "    then if X[2] <= 4.95\n",
            "      Class: 1, Num_samples: 34\n",
            "      else if X[0] <= 6.05\n",
            "        then if X[1] <= 2.45\n",
            "          Class: 2, Num_samples: 1\n",
            "          Class: 1, Num_samples: 1\n",
            "        Class: 2, Num_samples: 2\n",
            "    else if X[2] <= 4.85\n",
            "      then if X[1] <= 3.1\n",
            "        Class: 2, Num_samples: 3\n",
            "        Class: 1, Num_samples: 1\n",
            "      Class: 2, Num_samples: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_evaluation('wine', print_tree = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxgWZF2XM02H",
        "outputId": "dbd585e1-2f78-4883-e9f6-789405f63633"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin evaluation on wine dataset.\n",
            "\tSklearn Library Tree Prediction f1 score: 0.9561438923395444\n",
            "\tCART Prediction f1 score: 0.9555555555555556\n",
            "if X[6] <= 1.58\n",
            "  then if X[9] <= 3.825\n",
            "    Class: 1, Num_samples: 10\n",
            "    else if X[2] <= 2.06\n",
            "      Class: 1, Num_samples: 1\n",
            "      Class: 2, Num_samples: 36\n",
            "  else if X[12] <= 724.5\n",
            "    Class: 1, Num_samples: 42\n",
            "    else if X[0] <= 12.66\n",
            "      Class: 1, Num_samples: 3\n",
            "      Class: 0, Num_samples: 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_evaluation('breast_cancer', print_tree = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRHNQwdx_iNc",
        "outputId": "6cb1f04f-a35b-4832-d832-234113c9fadc"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin evaluation on breast_cancer dataset.\n",
            "\tSklearn Library Tree Prediction f1 score: 0.9373317757757117\n",
            "\tCART Prediction f1 score: 0.9373317757757117\n",
            "if X[23] <= 884.75\n",
            "  then if X[27] <= 0.13579999999999998\n",
            "    then if X[13] <= 35.71\n",
            "      then if X[21] <= 33.269999999999996\n",
            "        Class: 1, Num_samples: 228\n",
            "        else if X[20] <= 14.43\n",
            "          Class: 1, Num_samples: 11\n",
            "          Class: 0, Num_samples: 3\n",
            "      else if X[19] <= 0.003566\n",
            "        then if X[1] <= 16.08\n",
            "          Class: 1, Num_samples: 3\n",
            "          Class: 0, Num_samples: 5\n",
            "        Class: 1, Num_samples: 7\n",
            "    else if X[1] <= 20.299999999999997\n",
            "      then if X[13] <= 21.285\n",
            "        Class: 1, Num_samples: 8\n",
            "        else if X[12] <= 2.62\n",
            "          Class: 0, Num_samples: 8\n",
            "          Class: 1, Num_samples: 6\n",
            "      Class: 0, Num_samples: 15\n",
            "  else if X[1] <= 15.015\n",
            "    then if X[4] <= 0.097625\n",
            "      Class: 1, Num_samples: 4\n",
            "      Class: 0, Num_samples: 4\n",
            "    Class: 0, Num_samples: 124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regression_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsojJKDRFKtq",
        "outputId": "ea882cd8-bb72-4b43-99db-295101c73e06"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin evaluation on regression tasks.\n",
            "\tSklearn Library Tree Prediction MSE: 14318.652696196305\n",
            "\tCart Prediction MSE: 14318.652696196305\n",
            "if X[3] <= 48.0\n",
            "  then if X[4] <= 28.5\n",
            "    then if X[0] <= 28.5\n",
            "      then if X[2] <= 79.5\n",
            "        then if X[3] <= 25.0\n",
            "          Value: 304.2830404124651, Num_samples: 7\n",
            "          Value: 390.0190771722673, Num_samples: 5\n",
            "        else if X[4] <= 6.0\n",
            "          Value: 370.6246415136852, Num_samples: 2\n",
            "          Value: 516.1115516873718, Num_samples: 2\n",
            "      else if X[3] <= 29.0\n",
            "        then if X[3] <= 10.5\n",
            "          Value: 343.14456747748136, Num_samples: 1\n",
            "          Value: 484.54385930633106, Num_samples: 12\n",
            "        else if X[4] <= 17.0\n",
            "          Value: 533.2030063969042, Num_samples: 7\n",
            "          Value: 678.4869220609514, Num_samples: 4\n",
            "    else if X[4] <= 66.5\n",
            "      then if X[0] <= 55.5\n",
            "        then if X[3] <= 23.0\n",
            "          Value: 561.9072286110744, Num_samples: 15\n",
            "          Value: 663.0161108280506, Num_samples: 18\n",
            "        else if X[4] <= 43.5\n",
            "          Value: 685.319096417697, Num_samples: 12\n",
            "          Value: 797.9187472925098, Num_samples: 20\n",
            "      else if X[3] <= 9.5\n",
            "        then if X[0] <= 57.5\n",
            "          Value: 650.813552665739, Num_samples: 6\n",
            "          Value: 766.8939088168206, Num_samples: 5\n",
            "        else if X[0] <= 48.5\n",
            "          Value: 804.2333017836725, Num_samples: 19\n",
            "          Value: 914.8594650247883, Num_samples: 22\n",
            "  else if X[4] <= 60.5\n",
            "    then if X[4] <= 13.5\n",
            "      then if X[1] <= 37.5\n",
            "        then if X[2] <= 45.0\n",
            "          Value: 574.8035379732077, Num_samples: 7\n",
            "          Value: 705.8386244575611, Num_samples: 10\n",
            "        else if X[4] <= 7.5\n",
            "          Value: 704.5876428541123, Num_samples: 7\n",
            "          Value: 799.9147343529287, Num_samples: 8\n",
            "      else if X[2] <= 74.5\n",
            "        then if X[1] <= 55.5\n",
            "          Value: 781.1224590584657, Num_samples: 36\n",
            "          Value: 887.5238301675154, Num_samples: 42\n",
            "        else if X[0] <= 89.5\n",
            "          Value: 937.9095280262716, Num_samples: 20\n",
            "          Value: 1101.5948447180572, Num_samples: 6\n",
            "    else if X[3] <= 86.5\n",
            "      then if X[1] <= 57.0\n",
            "        then if X[2] <= 26.0\n",
            "          Value: 851.8604019132678, Num_samples: 7\n",
            "          Value: 989.9694769442518, Num_samples: 20\n",
            "        else if X[2] <= 76.5\n",
            "          Value: 1035.1327497757613, Num_samples: 17\n",
            "          Value: 1170.0336400305523, Num_samples: 4\n",
            "      else if X[4] <= 72.5\n",
            "        then if X[0] <= 48.0\n",
            "          Value: 1015.4280432367405, Num_samples: 6\n",
            "          Value: 1197.3984290513413, Num_samples: 2\n",
            "        else if X[1] <= 12.0\n",
            "          Value: 1101.9520632465258, Num_samples: 5\n",
            "          Value: 1265.917160684512, Num_samples: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evolutionary Hyperparameters Optimization\n",
        "\n",
        "In the task of using Decision Tree for classification or regression, parameter adjustment is very important. Appropriate parameters can improve model performance while avoiding overfitting. However, the traditional grid search method is inefficient. So, while writing CART, we created algorithms that use evolutionary algorithms to find optimal hyperparameters."
      ],
      "metadata": {
        "id": "FgvjBRjuNLAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define a function to get the performance of a parameters combination\n",
        "def perform(X_train, X_test, y_train, y_test, criterion, max_depth, min_samples, min_gain):\n",
        "  model = CART(criterion, max_depth, min_samples, min_gain)\n",
        "  model.fit(X_train, y_train)\n",
        "  pred = model.predict(X_test)\n",
        "  if criterion == 'mse':\n",
        "    return (-1) * np.mean((pred - y_test) ** 2)\n",
        "  else:\n",
        "    return f1_score(y_test, pred, average = 'weighted')\n",
        "\n",
        "# main function\n",
        "def evo_optimize(X_train, X_test, y_train, y_test, task, max_depth_list, min_samples_list, min_gain_active = False, min_gain_range = (0.01, 0.05), epochs = 10, plot = False):\n",
        "  # task can be 'regression' or 'classification'\n",
        "  # max_depth_list is the possible values for max_depth\n",
        "  # min_samples_list is the possible values for min_samples\n",
        "  # min_gain_active decides whether we will optimize min_gain\n",
        "  # min_gain_range is the range for possible values for min_gain\n",
        "  # epoch is the number of rounds of evolution\n",
        "\n",
        "  # initialization\n",
        "  print(\"Initializing......\")\n",
        "  if task == 'regression':\n",
        "    criterion = ['mse'] * 10\n",
        "  else:\n",
        "    criterion = rd.choices(['gini', 'entropy'], k = 10)\n",
        "  max_depth = rd.choices(max_depth_list, k = 10)\n",
        "  min_samples = rd.choices(min_samples_list, k = 10)\n",
        "  if min_gain_active == False:\n",
        "    min_gain = [None] * 10\n",
        "  else:\n",
        "    min_gain = [rd.uniform(min_gain_range[0], min_gain_range[1]) for _ in range(10)]\n",
        "  # generate 10 initial samples\n",
        "  env = list()\n",
        "  for i in range(10):\n",
        "    env.append([criterion[i], max_depth[i], min_samples[i], min_gain[i]])\n",
        "  del criterion, max_depth, min_samples, min_gain\n",
        "  # get the performances\n",
        "  performances = [perform(X_train, X_test, y_train, y_test, row[0], row[1], row[2], row[3]) for row in env]\n",
        "  # sort both env and performances\n",
        "  order = [i for i, _ in sorted(enumerate(performances), key = lambda x:x[1], reverse = True)]\n",
        "  env = [env[i] for i in order]\n",
        "  performances = [performances[i] for i in order]\n",
        "  # loop for pre-set generations\n",
        "  history = [performances[0]]\n",
        "  print(\"Initialization finished, begin evolving......\")\n",
        "  for epoch in tqdm(range(1, epochs + 1)):\n",
        "    env = env[0:4] # Eliminate the last six\n",
        "    performances = performances[0:4]\n",
        "    # Reproduce\n",
        "    new_env = list()\n",
        "    for i,j in [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]:\n",
        "      child = list()\n",
        "      for k in range(4):\n",
        "        prob = rd.random()\n",
        "        if prob < 0.35:\n",
        "          child.append(env[i][k])\n",
        "        elif prob < 0.7:\n",
        "          child.append(env[j][k])\n",
        "        else: # mutation probability is 30%\n",
        "          if (k == 0) and (task == 'regression'):\n",
        "            child.append('mse')\n",
        "          elif (k == 0) and (task == 'classification'):\n",
        "            child.append(rd.choice(['gini', 'entropy']))\n",
        "          elif k == 1:\n",
        "            child.append(rd.choice(max_depth_list))\n",
        "          elif k == 2:\n",
        "            child.append(rd.choice(min_samples_list))\n",
        "          elif (k == 3) and (min_gain_active == False):\n",
        "            child.append(None)\n",
        "          else:\n",
        "            child.append(rd.uniform(min_gain_range[0], min_gain_range[1]))\n",
        "      new_env.append(child)\n",
        "    # generate new performances, merge and sort\n",
        "    new_performances = [perform(X_train, X_test, y_train, y_test, row[0], row[1], row[2], row[3]) for row in new_env]\n",
        "    env.extend(new_env)\n",
        "    performances.extend(new_performances)\n",
        "    order = [i for i, value in sorted(enumerate(performances), key = lambda x:x[1], reverse = True)]\n",
        "    env = [env[i] for i in order]\n",
        "    performances = [performances[i] for i in order]\n",
        "    history.append(performances[0])\n",
        "  # print the results\n",
        "  print(f\"The optimal hyperparameter choice for this {task} task is:\")\n",
        "  print(f\"criterion: {env[0][0]}\")\n",
        "  print(f\"max_depth: {env[0][1]}\")\n",
        "  print(f\"min_samples: {env[0][2]}\")\n",
        "  print(f\"min_gain: {env[0][3]}\")\n",
        "  if task == 'classification':\n",
        "    print(f\"The final f1 score is {performances[0]}.\")\n",
        "    if plot == True:\n",
        "      plt.plot(history)\n",
        "      plt.xlabel('generation')\n",
        "      plt.ylabel('f1 score')\n",
        "      plt.title('f1 score history')\n",
        "      plt.show()\n",
        "  else:\n",
        "    history = list(np.array(history) * (-1))\n",
        "    print(f\"The final MSE is {(-1) * performances[0]}.\")\n",
        "    if plot == True:\n",
        "      plt.plot(history)\n",
        "      plt.xlabel('generation')\n",
        "      plt.ylabel('MSE')\n",
        "      plt.title('mse history')\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "h5jHEj2TN40L"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "We evaluate this algorithm on the same tasks."
      ],
      "metadata": {
        "id": "mDdoSk-HmS3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_iris()\n",
        "X, y = dataset.data, dataset.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 513)\n",
        "\n",
        "task = 'classification'\n",
        "max_depth_list = range(3, 13)\n",
        "min_samples_list = range(1, 6)\n",
        "min_gain_active = False\n",
        "\n",
        "evo_optimize(X_train, X_test, y_train, y_test, task, max_depth_list, min_samples_list, min_gain_active = False, min_gain_range = (0.01, 0.05), epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB5GErClmPvX",
        "outputId": "f19b6421-4c82-4c1e-9eb0-5321ed1d3576"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing......\n",
            "Initialization finished, begin evolving......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:01<00:00,  6.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal hyperparameter choice for this classification task is:\n",
            "criterion: gini\n",
            "max_depth: 6\n",
            "min_samples: 3\n",
            "min_gain: None\n",
            "The final f1 score is 0.9738894018672412.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls = CART(criterion = 'gini', max_depth = 6, min_samples = 3, min_gain = None)\n",
        "cls.fit(X_train, y_train)\n",
        "cls.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5WaHgiguDOA",
        "outputId": "ec2f2703-e1d3-458b-aef7-7c250b35fefe"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "if X[2] <= 2.45\n",
            "  Class: 0, Num_samples: 35\n",
            "  else if X[3] <= 1.65\n",
            "    then if X[2] <= 4.95\n",
            "      Class: 1, Num_samples: 34\n",
            "      Class: 2, Num_samples: 4\n",
            "    else if X[2] <= 4.85\n",
            "      Class: 2, Num_samples: 4\n",
            "      Class: 2, Num_samples: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_wine()\n",
        "X, y = dataset.data, dataset.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 513)\n",
        "\n",
        "task = 'classification'\n",
        "max_depth_list = range(3, 13)\n",
        "min_samples_list = range(1, 6)\n",
        "min_gain_active = False\n",
        "\n",
        "evo_optimize(X_train, X_test, y_train, y_test, task, max_depth_list, min_samples_list, min_gain_active = False, min_gain_range = (0.01, 0.05), epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZzLCnOSmt-o",
        "outputId": "175c3c9b-5067-4ca7-8b47-a575bd79ece1"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing......\n",
            "Initialization finished, begin evolving......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:14<00:00,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal hyperparameter choice for this classification task is:\n",
            "criterion: entropy\n",
            "max_depth: 10\n",
            "min_samples: 5\n",
            "min_gain: None\n",
            "The final f1 score is 0.9555555555555556.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls = CART(criterion = 'entropy', max_depth = 10, min_samples = 5, min_gain = None)\n",
        "cls.fit(X_train, y_train)\n",
        "cls.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnSsWgR8wVMU",
        "outputId": "b955672c-f454-425c-b2f8-a9257e214cbb"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "if X[6] <= 1.58\n",
            "  then if X[9] <= 3.825\n",
            "    Class: 1, Num_samples: 10\n",
            "    Class: 2, Num_samples: 37\n",
            "  else if X[12] <= 724.5\n",
            "    Class: 1, Num_samples: 42\n",
            "    Class: 0, Num_samples: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = np.random.randint(low = 1, high = 6, size = 5)\n",
        "c = np.random.randint(low = -50, high = 50)\n",
        "X = np.random.randint(low = 1, high = 101, size=(500, 5))\n",
        "error = np.random.normal(loc = 0, scale = 1, size = 500) * np.mean(X, axis=1)\n",
        "y = X[:, 0] * parameters[0]\n",
        "for i in [1, 2, 3, 4]:\n",
        "  y += X[:, i] * parameters[i]\n",
        "y += c\n",
        "y = y.astype('float64')\n",
        "y += error\n",
        "del parameters, c, error\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 513)\n",
        "\n",
        "task = 'regression'\n",
        "max_depth_list = range(3, 13)\n",
        "min_samples_list = range(1, 6)\n",
        "min_gain_active = False\n",
        "\n",
        "evo_optimize(X_train, X_test, y_train, y_test, task, max_depth_list, min_samples_list, min_gain_active = False, min_gain_range = (0.01, 0.05), epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhJ6FZWOuvxj",
        "outputId": "3b8f98ed-6087-4345-8d5b-83f377892aa1"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing......\n",
            "Initialization finished, begin evolving......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:35<00:00,  3.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal hyperparameter choice for this regression task is:\n",
            "criterion: mse\n",
            "max_depth: 8\n",
            "min_samples: 3\n",
            "min_gain: None\n",
            "The final MSE is 15044.028244783318.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg = CART(criterion = 'mse', max_depth = 8, min_samples = 3, min_gain = None)\n",
        "reg.fit(X_train, y_train)\n",
        "reg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7V_6pXCv-an",
        "outputId": "277cfcd4-e37d-4d6e-c3f4-cb5a55db6647"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "if X[3] <= 50.5\n",
            "  then if X[1] <= 47.0\n",
            "    then if X[3] <= 21.5\n",
            "      then if X[4] <= 64.0\n",
            "        then if X[1] <= 34.5\n",
            "          Value: 365.8835642749943, Num_samples: 14\n",
            "          Value: 453.65896395141596, Num_samples: 8\n",
            "        else if X[1] <= 15.5\n",
            "          Value: 462.84406176807073, Num_samples: 7\n",
            "          Value: 586.2087346723567, Num_samples: 4\n",
            "      else if X[4] <= 75.5\n",
            "        then if X[1] <= 26.5\n",
            "          then if X[4] <= 13.0\n",
            "            Value: 390.1760172012003, Num_samples: 3\n",
            "            Value: 496.28587182530094, Num_samples: 12\n",
            "          else if X[4] <= 23.0\n",
            "            then if X[4] <= 16.0\n",
            "              Value: 553.1898741140201, Num_samples: 5\n",
            "              Value: 474.949339540143, Num_samples: 4\n",
            "            else if X[0] <= 53.5\n",
            "              Value: 577.5577817112726, Num_samples: 4\n",
            "              Value: 695.8546096490811, Num_samples: 4\n",
            "        else if X[1] <= 31.5\n",
            "          then if X[3] <= 33.0\n",
            "            Value: 564.3653627595679, Num_samples: 4\n",
            "            Value: 668.1055979182543, Num_samples: 4\n",
            "          Value: 767.2678623606448, Num_samples: 9\n",
            "    else if X[4] <= 28.5\n",
            "      then if X[2] <= 31.0\n",
            "        Value: 489.5299371920341, Num_samples: 11\n",
            "        else if X[1] <= 63.5\n",
            "          Value: 537.2009836725812, Num_samples: 4\n",
            "          Value: 664.1894447572939, Num_samples: 16\n",
            "      else if X[3] <= 12.5\n",
            "        then if X[1] <= 70.0\n",
            "          Value: 593.9830633118147, Num_samples: 5\n",
            "          Value: 710.6037791096886, Num_samples: 4\n",
            "        else if X[2] <= 90.5\n",
            "          then if X[2] <= 31.5\n",
            "            Value: 730.6374375746113, Num_samples: 13\n",
            "            else if X[3] <= 28.5\n",
            "              then if X[1] <= 65.0\n",
            "                Value: 706.2005376281925, Num_samples: 4\n",
            "                Value: 759.1917478424602, Num_samples: 6\n",
            "              else if X[1] <= 71.5\n",
            "                Value: 833.852627798369, Num_samples: 10\n",
            "                Value: 932.3288495481107, Num_samples: 10\n",
            "          Value: 980.9062202653704, Num_samples: 6\n",
            "  else if X[1] <= 44.5\n",
            "    then if X[3] <= 82.5\n",
            "      then if X[2] <= 34.0\n",
            "        then if X[4] <= 45.0\n",
            "          then if X[3] <= 61.0\n",
            "            Value: 533.9317875108343, Num_samples: 5\n",
            "            Value: 604.0862613554011, Num_samples: 7\n",
            "          Value: 723.7193348530277, Num_samples: 11\n",
            "        else if X[4] <= 37.0\n",
            "          Value: 736.3167781628838, Num_samples: 18\n",
            "          else if X[1] <= 22.5\n",
            "            then if X[2] <= 83.5\n",
            "              Value: 789.4826546033246, Num_samples: 6\n",
            "              Value: 751.4342083511292, Num_samples: 3\n",
            "            else if X[0] <= 66.0\n",
            "              then if X[2] <= 77.5\n",
            "                Value: 850.8783312230247, Num_samples: 3\n",
            "                Value: 923.4007723086055, Num_samples: 3\n",
            "              else if X[0] <= 90.5\n",
            "                Value: 1025.4078509045996, Num_samples: 3\n",
            "                Value: 902.1062662230182, Num_samples: 4\n",
            "      else if X[4] <= 63.5\n",
            "        then if X[2] <= 89.5\n",
            "          Value: 811.6603520746003, Num_samples: 17\n",
            "          Value: 982.0041751551753, Num_samples: 3\n",
            "        Value: 984.9276529869917, Num_samples: 12\n",
            "    else if X[3] <= 71.5\n",
            "      then if X[4] <= 54.5\n",
            "        then if X[1] <= 65.5\n",
            "          then if X[4] <= 24.0\n",
            "            Value: 672.9638874528832, Num_samples: 4\n",
            "            Value: 769.4116243966213, Num_samples: 5\n",
            "          else if X[3] <= 62.5\n",
            "            Value: 807.5836288121878, Num_samples: 9\n",
            "            Value: 910.0961007756543, Num_samples: 5\n",
            "        else if X[1] <= 91.0\n",
            "          then if X[1] <= 59.5\n",
            "            Value: 910.6614874898545, Num_samples: 4\n",
            "            Value: 962.1640954083605, Num_samples: 12\n",
            "          Value: 1087.0938183876806, Num_samples: 3\n",
            "      else if X[4] <= 42.5\n",
            "        then if X[1] <= 75.5\n",
            "          Value: 894.2427476719363, Num_samples: 9\n",
            "          Value: 996.9055532026631, Num_samples: 14\n",
            "        else if X[1] <= 69.0\n",
            "          then if X[2] <= 83.5\n",
            "            then if X[4] <= 65.5\n",
            "              then if X[3] <= 90.5\n",
            "                Value: 907.3677005490881, Num_samples: 4\n",
            "                Value: 990.2379990191188, Num_samples: 3\n",
            "              else if X[3] <= 80.5\n",
            "                Value: 950.5658100429528, Num_samples: 3\n",
            "                Value: 1099.159148721766, Num_samples: 7\n",
            "            Value: 1142.4244529087925, Num_samples: 5\n",
            "          else if X[2] <= 33.5\n",
            "            Value: 1065.482933477668, Num_samples: 7\n",
            "            else if X[3] <= 91.0\n",
            "              Value: 1171.8949180174075, Num_samples: 9\n",
            "              Value: 1311.1944482218971, Num_samples: 6\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
